{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ride duration time prediction model\n",
    "In this notebook is going to be build and train a model to predict the ride duration time for the taxis of New York City, all the stages of a ML pipelines are going to be cover here, since the pre-processing of the data, the feature engineering and the modeling and training of the model.\n",
    "\n",
    "The datasets used in this notebook are the following:\n",
    "- Train dataset: [fhv_tripdata_2021-01.parquet](../data/fhv_tripdata_2021-01.parquet)\n",
    "- validation dataset: [fhv_tripdata_2021-02.parquet](../data/fhv_tripdata_2021-02.parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External imports\n",
    "\n",
    "# Data processing modules\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modeling modules\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n",
    "Let's make a fast exploratory data analysis to the dataset in order to find the right transformations and operations that have to be done in order to build a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropOff_datetime</th>\n",
       "      <th>PUlocationID</th>\n",
       "      <th>DOlocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00009</td>\n",
       "      <td>2021-01-01 00:27:00</td>\n",
       "      <td>2021-01-01 00:44:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>B00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00009</td>\n",
       "      <td>2021-01-01 00:50:00</td>\n",
       "      <td>2021-01-01 01:07:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>B00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00013</td>\n",
       "      <td>2021-01-01 00:01:00</td>\n",
       "      <td>2021-01-01 01:51:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>B00013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00037</td>\n",
       "      <td>2021-01-01 00:13:09</td>\n",
       "      <td>2021-01-01 00:21:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.0</td>\n",
       "      <td>None</td>\n",
       "      <td>B00037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00037</td>\n",
       "      <td>2021-01-01 00:38:31</td>\n",
       "      <td>2021-01-01 00:53:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.0</td>\n",
       "      <td>None</td>\n",
       "      <td>B00037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dispatching_base_num     pickup_datetime    dropOff_datetime  PUlocationID  \\\n",
       "0               B00009 2021-01-01 00:27:00 2021-01-01 00:44:00           NaN   \n",
       "1               B00009 2021-01-01 00:50:00 2021-01-01 01:07:00           NaN   \n",
       "2               B00013 2021-01-01 00:01:00 2021-01-01 01:51:00           NaN   \n",
       "3               B00037 2021-01-01 00:13:09 2021-01-01 00:21:26           NaN   \n",
       "4               B00037 2021-01-01 00:38:31 2021-01-01 00:53:44           NaN   \n",
       "\n",
       "   DOlocationID SR_Flag Affiliated_base_number  \n",
       "0           NaN    None                 B00009  \n",
       "1           NaN    None                 B00009  \n",
       "2           NaN    None                 B00013  \n",
       "3          72.0    None                 B00037  \n",
       "4          61.0    None                 B00037  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data from the .parquet file \n",
    "df = pd.read_parquet(\"../data/fhv_tripdata_2021-01.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's see the data types of the features of the dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dispatching_base_num              object\n",
       "pickup_datetime           datetime64[ns]\n",
       "dropOff_datetime          datetime64[ns]\n",
       "PUlocationID                     float64\n",
       "DOlocationID                     float64\n",
       "SR_Flag                           object\n",
       "Affiliated_base_number            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Let's see the data types of the features of the dataset:\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number shape of the dataset is: (1154112, 7)\n",
      "The number of NaN values in the <PUlocationID> feature is: 958267\n",
      "The number of NaN values in the <DOlocationID> feature is: 162220\n"
     ]
    }
   ],
   "source": [
    "# Let's print some important metrics \n",
    "print(f\"The number shape of the dataset is: {df.shape}\")\n",
    "print(f\"The number of NaN values in the <PUlocationID> feature is: {df[df.PUlocationID.isna()].shape[0]}\")\n",
    "print(f\"The number of NaN values in the <DOlocationID> feature is: {df[df.DOlocationID.isna()].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing \n",
    "Let's define a preprocessing pipeline function in order to read the .parquet files and extract the features needed in order to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a data preprocessing pipeline function \n",
    "def data_preprocessing_pipeline(data):\n",
    "    df = pd.read_parquet(data)\n",
    "    df.PUlocationID  = df.PUlocationID.apply(lambda x: -1 if (np.isnan(x)) else x)\n",
    "    df.DOlocationID  = df.DOlocationID.apply(lambda x: -1 if (np.isnan(x)) else x)\n",
    "    \n",
    "    df.pickup_datetime = pd.to_datetime(df.pickup_datetime)\n",
    "    df.dropOff_datetime = pd.to_datetime(df.dropOff_datetime)\n",
    "\n",
    "    df['duration'] = df.dropOff_datetime - df.pickup_datetime\n",
    "    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical = ['PUlocationID', 'DOlocationID']\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "\n",
    "    return df[[\"PUlocationID\",\"DOlocationID\",\"duration\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's preprocess the train and validation dataset\n",
    "df_train = data_preprocessing_pipeline(\"../data/fhv_tripdata_2021-01.parquet\")\n",
    "df_val = data_preprocessing_pipeline(\"../data/fhv_tripdata_2021-02.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "Let's do the feature engineering with the datasets, do the one-hot encoding to the categorical variables that are going to be use to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DictVectorizer object to do the one-hot encoding\n",
    "dv = DictVectorizer()\n",
    "\n",
    "train_dicts = df_train[[\"PUlocationID\",\"DOlocationID\"]].to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "val_dicts = df_val[[\"PUlocationID\",\"DOlocationID\"]].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable \n",
    "y_train = df_train.duration.values\n",
    "y_val = df_val.duration.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "Let's build and train a simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R^2 is: 0.16927348635061046\n",
      "Train - RMSE: 10.528519107212144\n",
      "Val - RMSE: 11.014283226749118\n"
     ]
    }
   ],
   "source": [
    "# Build and train the linear regression model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(f\"The R^2 is: {lr.score(X_train, y_train)}\")\n",
    "\n",
    "y_pred_train = lr.predict(X_train)\n",
    "y_pred_val = lr.predict(X_val)\n",
    "\n",
    "print(f\"Train - RMSE: {mean_squared_error(y_train, y_pred_train, squared=False)}\")\n",
    "print(f\"Val - RMSE: {mean_squared_error(y_val, y_pred_val, squared=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "with open('../models/lin_reg.bin', 'wb') as f_out:\n",
    "    pickle.dump((dv, lr), f_out)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7a4fee0164ac13b4e88cd7c8b0b3f09a6c0d6348c60b374e0be17ede752dc9d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
